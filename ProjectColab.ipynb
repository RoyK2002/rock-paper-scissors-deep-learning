{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17565,"status":"ok","timestamp":1754317841417,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"},"user_tz":240},"id":"2MsXntAFyoNI","outputId":"3afe3aac-fdf7-41a1-d87f-045ccc01f8c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"OOq5ja-SsrgH"},"source":["# Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzjQ0IifshmS"},"outputs":[],"source":["import torch\n","import gradio as gr\n","from torchvision import transforms\n","from PIL import Image\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"wOVtDzLh9GzX"},"source":["# Load Dataset (Train, Validation, Test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_Mf5ZXH5Ete"},"outputs":[],"source":["\n","\n","data_dir = '/content/drive/MyDrive/Deep Learning with pytorch/rps_dataset/Rock-Paper-Scissors'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((150, 150)),\n","    transforms.ToTensor()\n","])\n","\n","# Load datasets\n","train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n","val_data = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=transform)\n","test_data = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=transform)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"]},{"cell_type":"markdown","metadata":{"id":"xtuwjW7mNVTC"},"source":["This section loads the Rock-Paper-Scissors image dataset from Google Drive and prepares it for training, validation, and testing. It resizes all images to 150x150 pixels and converts them to tensors using `transforms`. The `ImageFolder` function organizes the data based on folder structure (train, validation, test), and `DataLoader` is used to load the data in batches of 32. Shuffling is applied to the training data to improve model generalization.\n"]},{"cell_type":"markdown","metadata":{"id":"JfN6cRpZ9W2S"},"source":["# Define the CNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z75COe5euXuj"},"outputs":[],"source":["\n","\n","class RPS_CNN(nn.Module):\n","    def __init__(self):\n","        super(RPS_CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.fc1 = nn.Linear(64 * 37 * 37, 128)\n","        self.fc2 = nn.Linear(128, 3)  # 3 classes: rock, paper, scissors\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))  # 150x150 -> 75x75\n","        x = self.pool(F.relu(self.conv2(x)))  # 75x75 -> 37x37\n","        x = x.view(-1, 64 * 37 * 37)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"xLpCE2LSNo4I"},"source":["This code defines a Convolutional Neural Network (CNN) named `RPS_CNN` for classifying images into rock, paper, or scissors. It has two convolutional layers with ReLU activation and max pooling to reduce image size. After feature extraction, the image is flattened and passed through two fully connected layers. The final layer outputs predictions for the three classes.\n"]},{"cell_type":"markdown","metadata":{"id":"Ql0zF3Cn9cb0"},"source":["# Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyPckWeT5H7x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754320514944,"user_tz":240,"elapsed":2277958,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"}},"outputId":"4b78bbfe-0bbc-4955-93f9-1cc450f259d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 43.3669, Accuracy: 80.16%\n","Epoch [2/10], Loss: 2.4939, Accuracy: 99.29%\n","Epoch [3/10], Loss: 0.4586, Accuracy: 100.00%\n","Epoch [4/10], Loss: 0.1593, Accuracy: 100.00%\n","Epoch [5/10], Loss: 0.0749, Accuracy: 100.00%\n","Epoch [6/10], Loss: 0.0418, Accuracy: 100.00%\n","Epoch [7/10], Loss: 0.0282, Accuracy: 100.00%\n","Epoch [8/10], Loss: 0.0217, Accuracy: 100.00%\n","Epoch [9/10], Loss: 0.0172, Accuracy: 100.00%\n","Epoch [10/10], Loss: 0.0142, Accuracy: 100.00%\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = RPS_CNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        correct += predicted.eq(labels).sum().item()\n","        total += labels.size(0)\n","\n","    acc = 100 * correct / total\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"9WmUPIVAOEyS"},"source":["This code trains the CNN model on the Rock-Paper-Scissors dataset. It uses the Adam optimizer and CrossEntropyLoss. The model runs for 10 epochs, and in each epoch, it processes all training images in batches, calculates loss and gradients, updates model weights, and tracks the accuracy. After each epoch, it prints the loss and training accuracy. The model is trained on GPU if available.\n"]},{"cell_type":"markdown","metadata":{"id":"Rvm3KIal9kpc"},"source":["# Evaluate on Validation Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-DRVUAk5MYP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754320553215,"user_tz":240,"elapsed":16387,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"}},"outputId":"d849b92e-ba31-4342-8e50-2f816e6f1d21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 81.82%\n"]}],"source":["model.eval()\n","val_correct = 0\n","val_total = 0\n","\n","with torch.no_grad():\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = outputs.max(1)\n","        val_correct += predicted.eq(labels).sum().item()\n","        val_total += labels.size(0)\n","\n","val_acc = 100 * val_correct / val_total\n","print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"ee2papBvOjt-"},"source":["This code evaluates the trained model on the validation set. It sets the model to evaluation mode (model.eval()), disables gradient calculations (with torch.no_grad()), and then predicts labels for the validation images. It compares the predictions with actual labels to compute the total correct predictions and calculates the validation accuracy as a percentage."]},{"cell_type":"markdown","metadata":{"id":"0UAd4Oi39mzl"},"source":["# Test Set Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hE3SZhuf89cC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754320739474,"user_tz":240,"elapsed":181522,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"}},"outputId":"c72d1886-262a-4a9e-9bfb-e78f8e57a8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 75.81%\n"]}],"source":["test_correct = 0\n","test_total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = outputs.max(1)\n","        test_correct += predicted.eq(labels).sum().item()\n","        test_total += labels.size(0)\n","\n","test_acc = 100 * test_correct / test_total\n","print(f\"Test Accuracy: {test_acc:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"P8UkvlgnOlh1"},"source":["This code evaluates the model on the test dataset. It disables gradient calculation for efficiency (`with torch.no_grad()`), feeds the test images to the model, and compares predicted labels with the true labels. The total correct predictions are tracked and used to calculate the test accuracy as a percentage, which is then printed.\n"]},{"cell_type":"markdown","metadata":{"id":"QyLzEeFmdb_I"},"source":["# Hyperparameter tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fAiI-2ma4WB"},"outputs":[],"source":["def train_model(model, train_loader, optimizer, criterion, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","            total += labels.size(0)\n","\n","        acc = 100 * correct / total\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIdoV8UTUQ5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754323277910,"user_tz":240,"elapsed":2399133,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"}},"outputId":"94b62fca-b2be-4624-f724-553eb625d3c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with learning rate = 0.001\n","Epoch [1/5], Loss: 54.0314, Accuracy: 76.15%\n","Epoch [2/5], Loss: 3.0983, Accuracy: 99.21%\n","Epoch [3/5], Loss: 0.5237, Accuracy: 99.96%\n","Epoch [4/5], Loss: 0.1770, Accuracy: 100.00%\n","Epoch [5/5], Loss: 0.0714, Accuracy: 100.00%\n","\n","Training with learning rate = 0.0005\n","Epoch [1/5], Loss: 77.1582, Accuracy: 58.29%\n","Epoch [2/5], Loss: 18.7126, Accuracy: 94.25%\n","Epoch [3/5], Loss: 2.2562, Accuracy: 99.60%\n","Epoch [4/5], Loss: 0.5203, Accuracy: 99.92%\n","Epoch [5/5], Loss: 0.2205, Accuracy: 100.00%\n","\n","Training with learning rate = 0.0001\n","Epoch [1/5], Loss: 65.9425, Accuracy: 66.59%\n","Epoch [2/5], Loss: 23.9922, Accuracy: 92.82%\n","Epoch [3/5], Loss: 7.3174, Accuracy: 99.05%\n","Epoch [4/5], Loss: 3.1539, Accuracy: 99.48%\n","Epoch [5/5], Loss: 1.6272, Accuracy: 99.88%\n"]}],"source":["for lr in [0.001, 0.0005, 0.0001]:\n","    print(f\"\\nTraining with learning rate = {lr}\")\n","    model = RPS_CNN().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    train_model(model, train_loader, optimizer, criterion, num_epochs=5)\n"]},{"cell_type":"markdown","metadata":{"id":"t0YZgYeSeY7p"},"source":["This section tests different learning rates (0.001, 0.0005, 0.0001) to improve model accuracy. A `train_model()` function was created to train the CNN using the specified optimizer and learning rate. For each value, a new model is trained for 5 epochs, and performance is printed. This helps identify which learning rate leads to faster convergence and higher accuracy. Among the tested values, **0.001 performed best**, reaching 100% accuracy in fewer epochs.\n"]},{"cell_type":"markdown","metadata":{"id":"haow-f9sF4us"},"source":["# Gardio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOjQZzrn8_KZ"},"outputs":[],"source":["# Load model (make sure it's in eval mode)\n","model.eval()\n","\n","# Define preprocessing\n","transform = transforms.Compose([\n","    transforms.Resize((150, 150)),\n","    transforms.ToTensor()\n","])\n","\n","# Define class names\n","class_names = ['paper', 'rock', 'scissors']\n","\n","\n","# Prediction function\n","def predict(image):\n","    image = transform(image).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        output = model(image)\n","        _, predicted = torch.max(output, 1)\n","    return class_names[predicted.item()]\n"]},{"cell_type":"markdown","metadata":{"id":"Oxmr64dHPAzi"},"source":["This code sets up a simple Gradio web app to predict whether an uploaded image is showing rock, paper, or scissors using the trained CNN model. The input image is resized and transformed into a tensor, then passed to the model for prediction. The predicted class (e.g., paper, rock, or scissors) is returned and displayed to the user."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TbGA_0_GRsl","colab":{"base_uri":"https://localhost:8080/","height":660},"executionInfo":{"status":"ok","timestamp":1754323966772,"user_tz":240,"elapsed":508805,"user":{"displayName":"Roy Kurien","userId":"15788998565591224483"}},"outputId":"8d37c921-3000-49a3-c82a-8522c09c02b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://239e77bcaefd380bc3.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://239e77bcaefd380bc3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://239e77bcaefd380bc3.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":12}],"source":["gr.Interface(fn=predict,\n","             inputs=gr.Image(type=\"pil\"),\n","             outputs=\"text\",\n","             title=\"Rock Paper Scissors Classifier\").launch(debug=True)\n"]},{"cell_type":"markdown","metadata":{"id":"3M3ro9lUPLGy"},"source":["This line creates and launches the Gradio interface. It takes an image input (in PIL format), passes it to the predict function, and returns the predicted label as text. The web app is titled \"Rock Paper Scissors Classifier\" and will open in a new tab when launched."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1-ZrNEtXiOkWS0wAY_TnziUUlI0NeWfKe","authorship_tag":"ABX9TyPXu4HtYIcpdCKGq5FK6Coi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}